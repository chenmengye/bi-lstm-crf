# bi-lstm-crf

## 简介

不同于英文自然语言处理，中文自然语言处理，例如语义分析、文本分类、词语蕴含等任务都需要预先进行分词。要将中文进行分割，直观的方式是通过为语句中的每一个字进行标记，以确定这个字是位于一个词的开头还是之中：

例如“**成功入侵民主党的电脑系统**”这句话，我们为其标注为：

```js
"成功 入侵  民主党 的 电脑系统"
 B I  B I  B I I  S  B I I I
```

其中`B`表示一个词语的开头，`I`表示非一个词语的开头，`S`表示单字成词。这样我们就能达到分词的效果。

对于句子这样的序列而言，要为其进行标注，常用的是使用Bi-LSTM卷积网络进行序列标注，如下图：

<center>
    <img src="./imgs/Bi-LSTM.png" width=400 title="Bi-LSTM 的序列标注">
    </img>
</center>

通过Bi-LSTM获得每个词所对应的所有标签的概率，取最大概率的标注即可获得整个标注序列，如上图序列`W0W1W2`的标注为`BIS`。但这样有可能会取得不合逻辑的标注序列，如`BS`、`SI`等。我们需要为其设定一些约束，如：

* B后只能是I
* S之后只能是B、S
* ...

而要做到这一点，我们可以在原有的模型基础之上，加上一个CRF层，该层的作用即是学习符号之间的约束（如上所述）。模型架构变为Embedding + Bi-LSTM + CRF，原理参考论文：https://arxiv.org/abs/1508.01991。

## 语料预处理

要训练模型，首先需要准备好语料，这里选用人民日报2014年的80万语料作为训练语料。语料格式如下：

```js
"人民网/nz 1月1日/t 讯/ng 据/p 《/w [纽约/nsf 时报/n]/nz 》/w 报道/v ，/w 美国/nsf 华尔街/nsf 股市/n 在/p 2013年/t 的/ude1 最后/f 一天/mq 继续/v 上涨/vn ，/w 和/cc [全球/n 股市/n]/nz 一样/uyy ，/w 都/d 以/p [最高/a 纪录/n]/nz 或/c 接近/v [最高/a 纪录/n]/nz 结束/v 本/rz 年/qt 的/ude1 交易/vn 。/w "
```

每一个词语使用空格分开后面使用POS标记词性，而本模型所需要的语料格式如下：

```js
李 B
全 I
福 I
今 B
年 I
6 B
3 I
岁 I
了 S

政 B
策 I
执 I
行 I
```

使用命令:
```xml
python preprocess_data.py <语料目录> bis.txt -a
```
可将原始的有词性标注的文档转换为使BIS（B:表示语句块的开始，I:表示非语句块的开始，S:表示单独成词）标注的文件`bis.txt`。

## 训练

可使用`train.py`进行模型的训练，详细使用方式见`train.py -h`。

### 使用字（词）向量

在训练时可以使用已训练的字（词）向量作为每一个字的表征，字（词）向量的格式如下：

```js
而 -0.037438 0.143471 0.391358 ...
个 -0.045985 -0.065485 0.251576 ...
以 -0.085605 0.081578 0.227135 ...
可以 0.012544 0.069829 0.117207 ...
第 -0.321195 0.065808 0.089396 ...
上 -0.186070 0.189417 0.265060 ...
之 0.037873 0.075681 0.239715 ...
于 -0.197969 0.018578 0.233496 ...
对 -0.115746 -0.025029 -0 ...
```

每一个行，为一个字（词）和它所对应的特征向量。

1. 使用预训练的字（词）向量：
    使用`train.py --embedding_file_path <字（词）向量文件>`利用预训练的字向量进行训练。
2. 汉字字（词）向量来源
    可从[https://github.com/Embedding/Chinese-Word-Vectors](https://github.com/Embedding/Chinese-Word-Vectors)获得字（词）向量。字（词）向量文件中每一行格式为一个字（词）与其对应的300维向量。

### 训练效果

1. 精度

    在迭代**15**次后，训练集精度达到了**99.3%**，验证集精度达到了**99.4%**。
2. 耗时
   
   训练时在**1070 ti** 上耗费**约2小时**左右。
   
3. 在人民日报2014年750篇新闻上的分词评估结果如下
    * 标准词数：196658个，正确词数：190436个，错误词数：5986个
    * 标准行数：5505，正确行数：3200，错误行数：2305
    * Recall: 0.968361
    * Precision: 0.969525
    * F MEASURE: 0.968943
    * ERR RATE: 0.030439

## 评估

使用与黄金标准文件进行对比的方式，进行评估。

1. 数据预处理

为了生成黄金标准文件和无分词标记的原始文件，可用下列命令：
```python
python score_preprocess.py --corups_dir <语料文件夹> \
--gold_file_path <生成的黄金标准文件路径> \
--restore_file_path <生成无标记的原始文件路径>
```

2. 读取无标记的原始文件，并进行分词，输出到文件：

```python
python predict.py -tf <要分割的文本文件的路径> -pf <保存分词结果的文件路径>
```

3. 生成评估结果：

执行`score.py`可生成评估文件，默认使用黄金分割文件`./score/gold.utf8`，使用模型分词后的文件`./score/pred_text.utf8`，评估结果保存到`prf_tmp.txt`中。

```py
def main():
    F = prf_score('./score/gold.utf8', './score/pred_text.utf8', 'prf_tmp.txt', 15)
```

## 分词

在模型训练完成后，可以使用命令：
```py
predict.py -s <语句>
```
进行分词，单独一句话不可超过150词，否则会截断。多句话可用空格分隔，具体使用方式可使用`predict.py -h`查看。

### 分词效果展示

1. 科技类

```py
python predict.py -s "目前多任务学习方法大致可以总结为两类，一是不同任务之间共享相同的参数，二是挖掘不同任务之间隐藏的共有数据特征。"
```
>  '目前', '多', '任务', '学习', '方法', '大致', '可以', '总结', '为', '两类', '，', '一', '是', '不同', '任务', '之间', '共享', '相同', '的', '参数', '，', '二', '是', '挖掘', '不同', '任务', '之间', '隐藏', '的', '共有', '数据', '特征', '。'

2. 新闻类

```py
python predict.py -s "美国司法部副部长罗森·施泰因（Rod Rosenstein）指，这些俄罗斯情报人员涉嫌利用电脑病毒或“钓鱼电邮”，成功入侵民主党的电脑系统，偷取民主党高层成员之间的电邮，另外也从美国一个州的电脑系统偷取了50万名美国选民的资料。"
```

 > '美国司法部', '副部长', '罗森·施泰因', '（', 'Rod', ' ', 'Rosenstein', '）', '指', '，', '这些', '俄罗斯', '情报', '人员', '涉嫌', '利用', '电脑病毒', '或', '“', '钓鱼', '电邮', '”', '，', '成功', '入侵', '民主党', '的', '电脑系统', '，', '偷取', '民主党', '高层', '成员', '之间', '的', '电邮', '，', '另外', '也', '从', '美国', '一个', '州', '的', '电脑系统', '偷取', '了', '50万名', '美国', '选民', '的', '资料', '。'


## 目录说明

1. model

在`model`文件夹中，包含使用人民日报80万语料和使用[百度百科训练得到的词向量](https://pan.baidu.com/s/1eeCS7uD3e_qVN8rPwmXhAw)迭代**15**次后获得的模型文件：
* model.final.h5 模型权重文件
* model.dict 处理语料后生产的词库
* model.cfg 模型的配置文件

2. corups

在`corups`文件夹中，包含人民日报80万语料的部分示例

3. score

在`score`文件夹中，包含用于评估分词效果的示例文件



## 附录

1. 分词语料库： https://pan.baidu.com/s/1-LLzKOJglP5W0VCVsI0efg 密码: krhr 
